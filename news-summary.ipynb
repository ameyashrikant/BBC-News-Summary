{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.compat.v1 import ConfigProto, InteractiveSession\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-28T04:33:57.424857Z","iopub.execute_input":"2022-08-28T04:33:57.425491Z","iopub.status.idle":"2022-08-28T04:33:59.840806Z","shell.execute_reply.started":"2022-08-28T04:33:57.425400Z","shell.execute_reply":"2022-08-28T04:33:59.839812Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"classes = os.listdir('../input/bbc-news-summary/BBC News Summary/News Articles')\nArticles_dir = '../input/bbc-news-summary/BBC News Summary/News Articles/'\nSummaries_dir = '../input/bbc-news-summary/BBC News Summary/Summaries/'\n\narticles = []\nsummaries = []\nfile_arr = []\nfor cls in classes:\n    files = os.listdir(Articles_dir + cls)\n    for file in files:\n        article_file_path = Articles_dir + cls + '/' + file\n        summary_file_path = Summaries_dir + cls + '/' + file\n        try:\n            with open (article_file_path,'r') as f:\n                articles.append('.'.join([line.rstrip() for line in f.readlines()]))\n            with open (summary_file_path,'r') as f:\n                summaries.append('.'.join([line.rstrip() for line in f.readlines()]))\n            file_arr.append(cls + '/' + file)\n        except:\n            pass\n            \ndataset = pd.DataFrame({'File_path':file_arr,'Articles': articles,'Summaries':summaries})\n# dataset['Articles'][0]\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:33:59.843421Z","iopub.execute_input":"2022-08-28T04:33:59.844100Z","iopub.status.idle":"2022-08-28T04:34:03.100887Z","shell.execute_reply.started":"2022-08-28T04:33:59.844059Z","shell.execute_reply":"2022-08-28T04:34:03.100009Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:03.103802Z","iopub.execute_input":"2022-08-28T04:34:03.104651Z","iopub.status.idle":"2022-08-28T04:34:03.111043Z","shell.execute_reply.started":"2022-08-28T04:34:03.104614Z","shell.execute_reply":"2022-08-28T04:34:03.110078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"contractions_dictionary = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:03.112674Z","iopub.execute_input":"2022-08-28T04:34:03.113389Z","iopub.status.idle":"2022-08-28T04:34:03.127484Z","shell.execute_reply.started":"2022-08-28T04:34:03.113349Z","shell.execute_reply":"2022-08-28T04:34:03.126482Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from string import digits\nimport re\n\ndef Filter(text):\n#     pattern = r'[0-9]'\n\n# # Match all digits in the string and replace them with an empty string\n#     text = re.sub(pattern, '', text)\n#     remove_digits = str.maketrans('', '', digits)\n#     text = text.translate(remove_digits)\n    text=text.lower()\n    text=' '.join([contractions_dictionary[i] if i in contractions_dictionary.keys() else i for i in text.split()])\n    text=re.sub(r'\\(.*\\)',\"\",text)\n    text=re.sub(\"'s\",\"\",text)\n    text=re.sub('\"','',text)\n    text=' '.join([i for i in text.split() if i.isalpha()])\n    text=re.sub('[^a-zA-Z]',\" \",text)\n    \n    return text\n\ndataset['File_path'] = dataset['File_path'].apply(Filter)\ndataset['Articles'] = dataset['Articles'].apply(Filter)\ndataset['Summaries'] = dataset['Summaries'].apply(Filter)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:03.131436Z","iopub.execute_input":"2022-08-28T04:34:03.131817Z","iopub.status.idle":"2022-08-28T04:34:03.935165Z","shell.execute_reply.started":"2022-08-28T04:34:03.131780Z","shell.execute_reply":"2022-08-28T04:34:03.934193Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:03.936752Z","iopub.execute_input":"2022-08-28T04:34:03.937183Z","iopub.status.idle":"2022-08-28T04:34:03.944107Z","shell.execute_reply.started":"2022-08-28T04:34:03.937131Z","shell.execute_reply":"2022-08-28T04:34:03.943010Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 3000\nembedding_dim = 100\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 2000\n\narticles_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\narticles_tokenizer.fit_on_texts(dataset['Articles'])\n\nsummaries_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\nsummaries_tokenizer.fit_on_texts(dataset['Summaries'])\n\n# word_index = tokenizer.word_index\n\ntraining_sequences = articles_tokenizer.texts_to_sequences(dataset['Articles'])\ntraining_x_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntraining_sequences = summaries_tokenizer.texts_to_sequences(dataset['Summaries'])\ntraining_y_padded = pad_sequences(training_sequences, maxlen=30, padding=padding_type, truncating=trunc_type)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:03.945890Z","iopub.execute_input":"2022-08-28T04:34:03.946267Z","iopub.status.idle":"2022-08-28T04:34:05.059760Z","shell.execute_reply.started":"2022-08-28T04:34:03.946232Z","shell.execute_reply":"2022-08-28T04:34:05.058565Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"training_x_padded.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:05.061078Z","iopub.execute_input":"2022-08-28T04:34:05.062329Z","iopub.status.idle":"2022-08-28T04:34:05.069763Z","shell.execute_reply.started":"2022-08-28T04:34:05.062284Z","shell.execute_reply":"2022-08-28T04:34:05.068506Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"NUM_TRAIN = 2224\narticle_map = dict(map(reversed, articles_tokenizer.word_index.items()))\nsummaries_map = dict(map(reversed, summaries_tokenizer.word_index.items()))\n\narticle = np.zeros((NUM_TRAIN,100,vocab_size))\nsumm = np.zeros((NUM_TRAIN,30,vocab_size))\nsumm_target = np.zeros((NUM_TRAIN,30,vocab_size))\nfor i,sequence in enumerate(training_x_padded):\n    for j,word in enumerate(sequence):\n        article[i,j,word ] = 1\n        \nfor i,sequence in enumerate(training_y_padded):\n    for j,word in enumerate(sequence):\n        summ[i,j,word] = 1\n        if j>0:\n            summ_target[i,j-1,word] = 1","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:05.071344Z","iopub.execute_input":"2022-08-28T04:34:05.071807Z","iopub.status.idle":"2022-08-28T04:34:06.079963Z","shell.execute_reply.started":"2022-08-28T04:34:05.071771Z","shell.execute_reply":"2022-08-28T04:34:06.078749Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"article.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:34:06.081759Z","iopub.execute_input":"2022-08-28T04:34:06.082180Z","iopub.status.idle":"2022-08-28T04:34:06.090106Z","shell.execute_reply.started":"2022-08-28T04:34:06.082125Z","shell.execute_reply":"2022-08-28T04:34:06.089157Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_x = article\ntraining_y = summ\ntraining_y_target = summ_target\n# testing_x_padded = np.array(testing_x_padded)\n# testing_y_padded = np.array(testing_y_padded)\n\nprint(training_x.shape)\nprint(training_y.shape)\nprint(training_y_target.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:36:38.320835Z","iopub.execute_input":"2022-08-28T04:36:38.321574Z","iopub.status.idle":"2022-08-28T04:36:38.327806Z","shell.execute_reply.started":"2022-08-28T04:36:38.321532Z","shell.execute_reply":"2022-08-28T04:36:38.326728Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"encoder_inputs = tf.keras.layers.Input(shape=(None, vocab_size))\nencoder = tf.keras.layers.LSTM(embedding_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n\nencoder_states = [state_h, state_c]\n\n\ndecoder_inputs = tf.keras.layers.Input(shape=(None, vocab_size))\ndecoder_lstm = tf.keras.layers.LSTM(embedding_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\nmodel = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:36:43.841780Z","iopub.execute_input":"2022-08-28T04:36:43.842836Z","iopub.status.idle":"2022-08-28T04:36:45.383245Z","shell.execute_reply.started":"2022-08-28T04:36:43.842796Z","shell.execute_reply":"2022-08-28T04:36:45.382154Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.fit([training_x,training_y],training_y_target,\n          epochs=300,\n          batch_size=64,\n          callbacks = [tf.keras.callbacks.CSVLogger('./training.csv')]\n          )","metadata":{"execution":{"iopub.status.busy":"2022-08-28T04:36:45.385114Z","iopub.execute_input":"2022-08-28T04:36:45.386321Z","iopub.status.idle":"2022-08-28T04:52:21.055481Z","shell.execute_reply.started":"2022-08-28T04:36:45.386276Z","shell.execute_reply":"2022-08-28T04:52:21.054391Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.save('model.h5')\nmodel.save_weights('./my_checkpoint')\n\nencoder_model = tf.keras.Model(encoder_inputs, encoder_states)\ndecoder_state_input_h = tf.keras.layers.Input(shape = (embedding_dim,))\ndecoder_state_input_c = tf.keras.layers.Input(shape = (embedding_dim,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T05:35:07.328738Z","iopub.execute_input":"2022-08-28T05:35:07.329364Z","iopub.status.idle":"2022-08-28T05:35:07.725979Z","shell.execute_reply.started":"2022-08-28T05:35:07.329320Z","shell.execute_reply":"2022-08-28T05:35:07.725002Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"## Visualising the Accuracy and loss plots\n\ncallback_csv = pd.read_csv('./training.csv')\nacc = callback_csv['accuracy']\nloss = callback_csv['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\n\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\n\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T05:31:25.666930Z","iopub.execute_input":"2022-08-28T05:31:25.667331Z","iopub.status.idle":"2022-08-28T05:31:26.379705Z","shell.execute_reply.started":"2022-08-28T05:31:25.667297Z","shell.execute_reply":"2022-08-28T05:31:26.378688Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Define Decode Sequence\ndef decode_sequence(input_seq):\n    #Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    #Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, 3000))\n    #Get the first character of target sequence with the start character.\n    target_seq[0, 0, summaries_tokenizer.word_index['<OOV>']] = 1.\n\n    #Sampling loop for a batch of sequences\n    #(to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        #Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = summaries_map[sampled_token_index]\n        decoded_sentence += ' '+ sampled_char\n        \n        #Exit condition: either hit max length\n        #or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > 70):\n            stop_condition = True\n\n        #Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, 3000))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        #Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T05:35:13.448812Z","iopub.execute_input":"2022-08-28T05:35:13.449313Z","iopub.status.idle":"2022-08-28T05:35:13.457524Z","shell.execute_reply.started":"2022-08-28T05:35:13.449270Z","shell.execute_reply":"2022-08-28T05:35:13.456208Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for seq_index in range(10):\n    input_seq = training_x[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', dataset['Articles'][seq_index])\n    print('Decoded sentence:', decoded_sentence)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T05:35:16.278621Z","iopub.execute_input":"2022-08-28T05:35:16.278988Z","iopub.status.idle":"2022-08-28T05:35:23.503606Z","shell.execute_reply.started":"2022-08-28T05:35:16.278958Z","shell.execute_reply":"2022-08-28T05:35:23.502297Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}